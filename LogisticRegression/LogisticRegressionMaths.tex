\documentclass[]{report}

\voffset=-1.5cm
\oddsidemargin=0.0cm
\textwidth = 480pt

\usepackage{framed}
\usepackage{subfiles}
\usepackage{enumerate}
\usepackage{graphics}
\usepackage{newlfont}
\usepackage{eurosym}
\usepackage{amsmath,amsthm,amsfonts}
\usepackage{amsmath}
\usepackage{color}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}
\begin{document}
\author{Kevin O'Brien}
\title{Cluster Analysis - Distance Measures and Proximity Matrices}
\tableofcontents
\subsection{The Logit}
The convention for binomial logistic regression is to code the
dependent class of greatest interest as 1 and the other class as 0, because the coding will
affect the odds ratios and slope estimates.

The logit(p) is the log (to base e) of the odds ratio or likelihood ratio that the dependent
variable is 1. In symbols it is defined as:
\[ logit(p) = ln \left(\frac{p}{(1-p)}\right) \]

Whereas p can only range from 0 to 1, logit(p) scale ranges from negative infinity to positive
infinity and is symmetrical around the logit of 0.5 (which is zero)

\subsection{The Logistic Regression Equation}
The form of the logistic regression equation is:
\[ logit[p(x)] =  log \frac{p(x)}{1-p(x)}  = b_0 + b_1x_1 + b_2x_2 + b_3x_3 + \ldots \]

This looks just like a linear regression and although logistic regression finds a best
fitting equation, just as linear regression does, the principles on which it does so are
rather different. Instead of using a least-squared deviations criterion for the best fit, it
uses a maximum likelihood method, which maximizes the probability of getting the
observed results given the fitted regression coefficients. A consequence of this is that the
goodness of fit and overall significance statistics used in logistic regression are different
from those used in linear regression.

The probability that a case is in a particular category,p, can be calculated with the following formula (which is simply another rearrangement of the previous formula).

\[p = \frac{exp(b_0 + b_1x_1 + b_2x_2 + b_3x_3 + \ldots)}{1 + exp(b_0 + b_1x_1 + b_2x_2 + b_3x_3 + \ldots)}\]


The logit, or logistic regression model, is a form of regression analysis that takes data and tries to predict outcomes with it, such as basing a customer's propensity towards purchasing a new car or not on his or her income, age, and family size. The probit model is also a form of linear regression with a simpler binary component to it that tries to predict the maximum likelihood of one of two outcomes, such as whether an individual is married or not based on available probit regression data.

%--------------------------------------------------------------------------------------%
\newpage
\section{Logistic Regression: Odds Ratios and Log-Odds}
Suppose that in a sample of 100 men, 90 drank wine in the previous week, while in a sample of 100 women only 20 drank wine in the same period. The odds of a man drinking wine are 90 to 10, or 9:1, while the odds of a woman drinking wine are only 20 to 80, or 1:4 = 0.25:1. The odds ratio is thus 9/0.25, or 36, showing that men are much more likely to drink wine than women. The detailed calculation is:

\[ { 0.9/0.1 \over 0.2/0.8}=\frac{\;0.9\times 0.8\;}{\;0.1\times 0.2\;} ={0.72 \over 0.02} = 36 \]

This example also shows how odds ratios are sometimes sensitive in stating relative positions: in this sample men are 90/20 = 4.5 times more likely to have drunk wine than women, but have 36 times the odds. 


The logarithm of the odds ratio, the difference of the logits of the probabilities, tempers this effect, and also makes the measure symmetric with respect to the ordering of groups. For example, using natural logarithms, an odds ratio of 36/1 maps to 3.584, and an odds ratio of 1/36 maps to -3.584.


\section{Logistic Regression: Logits}
%http://data.princeton.edu/wws509/notes/c3.pdf

The logit transformation is given by the following formula: 
\[ \eta_i = \mbox{logit}(\pi_i)  = \mbox{log}\left( \frac{\pi_i}{1- \pi_i} \right) \]

To inverse of the logit transformation is given by the following formula: 
\[ \pi_i = \mbox{logit}^{-1}(\eta_i)  =  \frac{e^{\eta_i}}{1 + e^{\eta_i}} \]

%---------------------------%
\subsection{Example 1}
Given that $\pi_i = 0.2$, compute $\eta_i$.

\[ \eta_i = \mbox{log}\left( \frac{0.2}{1-0.2} \right)= \mbox{log}\left( \frac{0.2}{0.8} \right)\] 

\[ \eta_i =  \mbox{log}(0.25) =-1.386 \]

%---------------------------%
\subsection{Example 2}
Given that $\eta_i = 2.3$, compute $\pi_i$.

\[ \pi_i  =  \frac{e^{2.3}}{1 + e^{2.3}} = \frac{9.974}{1 + 9.974} = 0.908 \]

%--------------------------------------------------------------------------------------%
\section{Logistic Regression}
In logistic regression, the logit may be computed in a manner similar to linear regression:
\[ \eta_i = \beta_0 + \beta_1x_1 + \beta_2x_2 + \ldots  \]

%---------------------------%
\subsection{Example 2}
Let us suppose that the probability of survival of a marine species of fauna is dependent on pollution, depth and water temperature. Suppose the logit for the logistic regression was computed as follows:
\[ \eta_i = 0.14 + 0.76x_1 - 0.093x_2 + 1.2x_3  \]
\begin{center}
\begin{tabular}{|c|c|c|}
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
Variables & case 1 & case 2 \\ \hline
Pollution($x_1$) & 6.0 & 1.9 \\
Depth ($x_2$)& 51 & 99 \\
Temp ($x_3$) & 3.0 & 2.9 \\
  \hline
\end{tabular}
\end{center}
Compute the probability of success for both case 1 and case 2.

\begin{itemize}
\item case 1$ \eta_1 = 0.14 + (0.76 \times 6)	- (0.093\times 51) + (1.2\times 3) = 3.557$
\item case 2$ \eta_2 = 0.14 + (0.76 \times 1.9)	- (0.093\times 99) + (1.2\times 2.9) = -4.143$
\end{itemize}

The probabilities for success are therefore:
\[ \pi_1  =  \frac{e^{3.557}}{1 + e^{3.557}} = \frac{35.057}{1 + 35.057} = 0.972 \]
\[ \pi_2  =  \frac{e^{-4.143}}{1 + e^{-4.143}} = \frac{0.0158}{1 + 0.0158} = 0.0156 \]

\newpage


\section*{Odds Ratio - Example}

Suppose that in a sample of 100 men, 90 drank wine in the previous week, while in a sample of 100 women only 20 drank wine in the same period. The odds of a man drinking wine are 90 to 10, or 9:1, while the odds of a woman drinking wine are only 20 to 80, or 1:4 = 0.25:1. The odds ratio is thus 9/0.25, or 36, showing that men are much more likely to drink wine than women. The detailed calculation is:
\[{ 0.9/0.1 \over 0.2/0.8}=\frac{\;0.9\times 0.8\;}{\;0.1\times 0.2\;} ={0.72 \over 0.02} = 36.\]
This example also shows how odds ratios are sometimes sensitive in stating relative positions: in this sample men are 90/20 = 4.5 times more likely to have drunk wine than women, but have 36 times the odds. The logarithm of the odds ratio, the difference of the logits of the probabilities, tempers this effect, and also makes the measure symmetric with respect to the ordering of groups. For example, using natural logarithms, an odds ratio of 36/1 maps to 3.584, and an odds ratio of 1/36 maps to −3.584.

\section*{What is an odds ratio?}


An odds ratio (OR) is a measure of association between an exposure and an outcome. The OR represents the odds that an outcome will occur given a particular exposure, compared to the odds of the outcome occurring in the absence of that exposure. Odds ratios are most commonly used in case-control studies, however they can also be used in cross-sectional and cohort study designs as well (with some modifications and/or assumptions).

\subsection*{Odds ratios and logistic regression}
When a logistic regression is calculated, the regression coefficient (b1) is the estimated increase in the log odds of the outcome per unit increase in the value of the exposure. In other words, the exponential function of the regression coefficient (eb1) is the odds ratio associated with a one-unit increase in the exposure.

%---------------------------------------------------------------%
\subsection{When is it used?}
Odds ratios are used to compare the relative odds of the occurrence of the outcome of interest (e.g. disease or disorder), given exposure to the variable of interest (e.g. health characteristic, aspect of medical history). The odds ratio can also be used to determine whether a particular exposure is a risk factor for a particular outcome, and to compare the magnitude of various risk factors for that outcome.

\begin{itemize}
\item OR$=1$ Exposure does not affect odds of outcome
\item OR$>1$ Exposure associated with higher odds of outcome
\item OR$<1$ Exposure associated with lower odds of outcome
\end{itemize}
%---------------------------------------------------------------%

\subsection{What about confidence intervals?}
The 95\% confidence interval (CI) is used to estimate the precision of the OR. A large CI indicates a low level of precision of the OR, whereas a small CI indicates a higher precision of the OR. It is important to note however, that unlike the p value, the 95\% CI does not report a measure’s statistical significance. In practice, the 95\% CI is often used as a proxy for the presence of statistical significance if it does not overlap the null value (e.g. OR=1). Nevertheless, it would be inappropriate to interpret an OR with 95\% CI that spans the null value as indicating evidence for lack of association between the exposure and outcome.

\end{document}

