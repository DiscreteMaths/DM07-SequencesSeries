<<<<<<< HEAD



\section{Training and validation}
%http://www.jmp.com/support/help/Validation_2.shtml
Using Validation and Test Data

%When you have sufficient data, you can subdivide your data into three parts called the training, validation, and test data. During the selection process, models are fit on the training data, and the prediction error for the models so obtained is found by using the validation data. This prediction error on the validation data can be used to decide when to terminate the selection process or to decide what effects to include as the selection process proceeds. Finally, once a selected model has been obtained, the test set can be used to assess how the selected model generalizes on data that played no role in selecting the model.

In some cases you might want to use only training and test data. For example, you might decide to use an information criterion to decide what effects to include and when to terminate the selection process. In this case no validation data are required, but test data can still be useful in assessing the predictive performance of the selected model. In other cases you might decide to use validation data during the selection process but forgo assessing the selected model on test data. 
%Hastie, Tibshirani, and Friedman (2001) note that it is difficult to give a general rule on how many observations you should assign to each role. They note that a typical split might be 50\% for training and 25% each for validation and testing.


\subsection{Cross-Validation and Testing}
%====================================================%
\begin{itemize}
	\item In order to build the best possible mode, we will split our training data into two parts: a training set and a test set. 
	
	\item 	The general idea is as follows. The model parameters (the regression coefficients) are learned using the training set as above. 
	\item The error is evaluated on the test set, and the meta-parameters are adjusted so that this cross-validation error is minimized. 

\end{itemize}	
\subsection{Cross Validation}
%-------------------------------------------------------------------------------------%
\begin{itemize}	
%	\item 	
%	
%	\item	The cross validated set of data is a more honest presentation of the power of the
%	discriminant function than that provided by the original classifications and often produces
%	a poorer outcome. 
	\item The cross validation is often termed a jack-knife classification, in that
	it successively classifies \textbf{all cases but one} to develop a predictive model and then
	categorizes the case that was left out. This process is repeated with each case left out in
	turn.This is known as leave-1-out cross validation. 
	
	\item 	This cross validation produces a more reliable function. The argument behind it is that
	one should not use the case you are trying to predict as part of the categorization process.
\end{itemize}



%-----------------------------------------------------------------------------------%




\subsection{Cross Validation}
\begin{itemize}
\item In a prediction problem, a model is usually given a dataset of known data 
on which training is run (\textit{training dataset}), and a dataset of unknown data (or \textit{first seen data/ testing dataset}) against which testing the model is performed.
\item Cross-validation is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice, with unseen data.
\item The goal of cross validation is to define a dataset to ``test" the model in the training phase, in order to limit problems like overfitting, give an insight on how the model will generalize to an independent data set (i.e., an unknown dataset, for instance from a real problem), etc.
\item Cross-validation is important in guarding against testing hypotheses suggested by the data (called ``\textbf{\textit{Type III errors}}"), especially where further samples 
are hazardous, costly or impossible to collect 
\end{itemize}
\subsection*{K-fold Cross Validation}
\begin{itemize}
\item In k-fold cross-validation, the original data set is randomly partitioned into $k$ equally sized subsamples (e.g. 10 samples).
 
\item Of the $k$ subsamples, a single subsample is retained as the testing data for testing the model, and the remaining k - 1 subsamples are used as training data. 
\item The cross-validation process is then repeated k times (the folds), with each of the $k$ subsamples used exactly once as the test data. \item The $k$ results from the folds can then be averaged (or otherwise combined) to produce a single estimation.
\item The advantage of this method over repeated random sub-sampling is that all observations are used for both training and testing, and each observation is used for testing exactly once. 
\end{itemize}
%\subsection*{Choosing K - Bias and Variance}
%In general, when using k-fold cross validation, it seems to be the case that:
%\begin{itemize}
%\item A larger k will produce an estimate with smaller bias but potentially higher variance (on top of being computationally expensive)
%\item A smaller k will lead to a smaller variance but may lead to a a biased estimate.
%\end{itemize}
\newpage

\subsection*{Leave-One-Out Cross-Validation}
\begin{itemize}
\item As the name suggests, \textbf{leave-one-out cross-validation}  \textbf{(LOOCV)} involves using a single observation from the original sample as the validation data, and the remaining observations as the training data. 
\item This is repeated such that each observation in the sample is used once as the validation data. 
\item This is the same as a K-fold cross-validation with K being equal to the number of observations in the original sampling, i.e. \textbf{K=n}.
\end{itemize}



%-------------------------------------------------------%


Standard Data Partition 

Standard Data Partition 
\section{Data Partitioning}
Most data mining projects use large volumes of data. Before building a model, typically you partition the data using a partition utility. Partitioning yields mutually exclusive datasets: a training dataset, a validation dataset and a test dataset.
\subsection*{Training Set}
The training dataset is used to train or build a model. For example, in a linear regression, the training dataset is used to fit the linear regression model, i.e. to compute the regression coefficients. In a neural network model, the training dataset is used to obtain the network weights.
\subsection*{Validation Set}
Once a model is built on training data, you need to find out the accuracy of the model on unseen data. For this, the model should be used on a dataset that was not used in the training process -- a dataset where you know the actual value of the target variable. The discrepancy between the actual value and the predicted value of the target variable is the error in prediction. Some form of average error (MSE of average % error) measures the overall accuracy of the model.
If you were to use the training data itself to compute the accuracy of the model fit, you would get an overly optimistic estimate of the accuracy of the model. This is because the training or model fitting process ensures that the accuracy of the model for the training data is as high as possible -- the model is specifically suited to the training data. To get a more realistic estimate of how the model would perform with unseen data, you need to set aside a part of the original data and not use it in the training process. This dataset is known as the validation dataset. After fitting the model on the training dataset, you should test its performance on the validation dataset.
\subsection*{Test Set}
The validation dataset is often used to fine-tune models. For example, you might try out neural network models with various architectures and test the accuracy of each on the validation dataset to choose among the competing architectures. In such a case, when a model is finally chosen, its accuracy with the validation dataset is still an optimistic estimate of how it would perform with unseen data. This is because the final model has come out as the winner among the competing models based on the fact that its accuracy with the validation dataset is highest. Thus, you need to set aside yet another portion of data which is used neither in training nor in validation. This set is known as the test dataset. The accuracy of the model on the test data gives a realistic estimate of the performance of the model on completely unseen data.

\end{document}

=======



\section{Training and validation}
%http://www.jmp.com/support/help/Validation_2.shtml
Using Validation and Test Data

%When you have sufficient data, you can subdivide your data into three parts called the training, validation, and test data. During the selection process, models are fit on the training data, and the prediction error for the models so obtained is found by using the validation data. This prediction error on the validation data can be used to decide when to terminate the selection process or to decide what effects to include as the selection process proceeds. Finally, once a selected model has been obtained, the test set can be used to assess how the selected model generalizes on data that played no role in selecting the model.

In some cases you might want to use only training and test data. For example, you might decide to use an information criterion to decide what effects to include and when to terminate the selection process. In this case no validation data are required, but test data can still be useful in assessing the predictive performance of the selected model. In other cases you might decide to use validation data during the selection process but forgo assessing the selected model on test data. 
%Hastie, Tibshirani, and Friedman (2001) note that it is difficult to give a general rule on how many observations you should assign to each role. They note that a typical split might be 50\% for training and 25% each for validation and testing.


\subsection{Cross-Validation and Testing}
%====================================================%
\begin{itemize}
	\item In order to build the best possible mode, we will split our training data into two parts: a training set and a test set. 
	
	\item 	The general idea is as follows. The model parameters (the regression coefficients) are learned using the training set as above. 
	\item The error is evaluated on the test set, and the meta-parameters are adjusted so that this cross-validation error is minimized. 

\end{itemize}	
\subsection{Cross Validation}
%-------------------------------------------------------------------------------------%
\begin{itemize}	
%	\item 	
%	
%	\item	The cross validated set of data is a more honest presentation of the power of the
%	discriminant function than that provided by the original classifications and often produces
%	a poorer outcome. 
	\item The cross validation is often termed a jack-knife classification, in that
	it successively classifies \textbf{all cases but one} to develop a predictive model and then
	categorizes the case that was left out. This process is repeated with each case left out in
	turn.This is known as leave-1-out cross validation. 
	
	\item 	This cross validation produces a more reliable function. The argument behind it is that
	one should not use the case you are trying to predict as part of the categorization process.
\end{itemize}



%-----------------------------------------------------------------------------------%




\subsection{Cross Validation}
\begin{itemize}
\item In a prediction problem, a model is usually given a dataset of known data 
on which training is run (\textit{training dataset}), and a dataset of unknown data (or \textit{first seen data/ testing dataset}) against which testing the model is performed.
\item Cross-validation is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice, with unseen data.
\item The goal of cross validation is to define a dataset to ``test" the model in the training phase, in order to limit problems like overfitting, give an insight on how the model will generalize to an independent data set (i.e., an unknown dataset, for instance from a real problem), etc.
\item Cross-validation is important in guarding against testing hypotheses suggested by the data (called ``\textbf{\textit{Type III errors}}"), especially where further samples 
are hazardous, costly or impossible to collect 
\end{itemize}
\subsection*{K-fold Cross Validation}
\begin{itemize}
\item In k-fold cross-validation, the original data set is randomly partitioned into $k$ equally sized subsamples (e.g. 10 samples).
 
\item Of the $k$ subsamples, a single subsample is retained as the testing data for testing the model, and the remaining k - 1 subsamples are used as training data. 
\item The cross-validation process is then repeated k times (the folds), with each of the $k$ subsamples used exactly once as the test data. \item The $k$ results from the folds can then be averaged (or otherwise combined) to produce a single estimation.
\item The advantage of this method over repeated random sub-sampling is that all observations are used for both training and testing, and each observation is used for testing exactly once. 
\end{itemize}
%\subsection*{Choosing K - Bias and Variance}
%In general, when using k-fold cross validation, it seems to be the case that:
%\begin{itemize}
%\item A larger k will produce an estimate with smaller bias but potentially higher variance (on top of being computationally expensive)
%\item A smaller k will lead to a smaller variance but may lead to a a biased estimate.
%\end{itemize}
\newpage

\subsection*{Leave-One-Out Cross-Validation}
\begin{itemize}
\item As the name suggests, \textbf{leave-one-out cross-validation}  \textbf{(LOOCV)} involves using a single observation from the original sample as the validation data, and the remaining observations as the training data. 
\item This is repeated such that each observation in the sample is used once as the validation data. 
\item This is the same as a K-fold cross-validation with K being equal to the number of observations in the original sampling, i.e. \textbf{K=n}.
\end{itemize}



%-------------------------------------------------------%


Standard Data Partition 

Standard Data Partition 
\section{Data Partitioning}
Most data mining projects use large volumes of data. Before building a model, typically you partition the data using a partition utility. Partitioning yields mutually exclusive datasets: a training dataset, a validation dataset and a test dataset.
\subsection*{Training Set}
The training dataset is used to train or build a model. For example, in a linear regression, the training dataset is used to fit the linear regression model, i.e. to compute the regression coefficients. In a neural network model, the training dataset is used to obtain the network weights.
\subsection*{Validation Set}
Once a model is built on training data, you need to find out the accuracy of the model on unseen data. For this, the model should be used on a dataset that was not used in the training process -- a dataset where you know the actual value of the target variable. The discrepancy between the actual value and the predicted value of the target variable is the error in prediction. Some form of average error (MSE of average % error) measures the overall accuracy of the model.
If you were to use the training data itself to compute the accuracy of the model fit, you would get an overly optimistic estimate of the accuracy of the model. This is because the training or model fitting process ensures that the accuracy of the model for the training data is as high as possible -- the model is specifically suited to the training data. To get a more realistic estimate of how the model would perform with unseen data, you need to set aside a part of the original data and not use it in the training process. This dataset is known as the validation dataset. After fitting the model on the training dataset, you should test its performance on the validation dataset.
\subsection*{Test Set}
The validation dataset is often used to fine-tune models. For example, you might try out neural network models with various architectures and test the accuracy of each on the validation dataset to choose among the competing architectures. In such a case, when a model is finally chosen, its accuracy with the validation dataset is still an optimistic estimate of how it would perform with unseen data. This is because the final model has come out as the winner among the competing models based on the fact that its accuracy with the validation dataset is highest. Thus, you need to set aside yet another portion of data which is used neither in training nor in validation. This set is known as the test dataset. The accuracy of the model on the test data gives a realistic estimate of the performance of the model on completely unseen data.

\end{document}

>>>>>>> 1ef437d6e5c26def8a03ded0973347918b682cb0
