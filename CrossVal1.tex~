

\subsection{Cross Validation}
\begin{itemize}
\item In a prediction problem, a model is usually given a dataset of known data 
on which training is run (\textit{training dataset}), and a dataset of unknown data (or \textit{first seen data/ testing dataset}) against which testing the model is performed.
\item Cross-validation is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice, with unseen data.
\item The goal of cross validation is to define a dataset to ``test" the model in the training phase, in order to limit problems like overfitting, give an insight on how the model will generalize to an independent data set (i.e., an unknown dataset, for instance from a real problem), etc.
\item Cross-validation is important in guarding against testing hypotheses suggested by the data (called ``\textbf{\textit{Type III errors}}"), especially where further samples 
are hazardous, costly or impossible to collect 
\end{itemize}
\subsection*{K-fold Cross Validation}
\begin{itemize}
\item In k-fold cross-validation, the original data set is randomly partitioned into $k$ equally sized subsamples (e.g. 10 samples).
 
\item Of the $k$ subsamples, a single subsample is retained as the testing data for testing the model, and the remaining k - 1 subsamples are used as training data. 
\item The cross-validation process is then repeated k times (the folds), with each of the $k$ subsamples used exactly once as the test data. \item The $k$ results from the folds can then be averaged (or otherwise combined) to produce a single estimation.
\item The advantage of this method over repeated random sub-sampling is that all observations are used for both training and testing, and each observation is used for testing exactly once. 
\end{itemize}
%\subsection*{Choosing K - Bias and Variance}
%In general, when using k-fold cross validation, it seems to be the case that:
%\begin{itemize}
%\item A larger k will produce an estimate with smaller bias but potentially higher variance (on top of being computationally expensive)
%\item A smaller k will lead to a smaller variance but may lead to a a biased estimate.
%\end{itemize}
\newpage

\subsection*{Leave-One-Out Cross-Validation}
\begin{itemize}
\item As the name suggests, \textbf{leave-one-out cross-validation}  \textbf{(LOOCV)} involves using a single observation from the original sample as the validation data, and the remaining observations as the training data. 
\item This is repeated such that each observation in the sample is used once as the validation data. 
\item This is the same as a K-fold cross-validation with K being equal to the number of observations in the original sampling, i.e. \textbf{K=n}.
\end{itemize}


\end{document}
