
\documentclass[]{report}

\voffset=-1.5cm
\oddsidemargin=0.0cm
\textwidth = 480pt

\usepackage{framed}
\usepackage{subfiles}
\usepackage{graphics}
\usepackage{newlfont}
\usepackage{eurosym}
\usepackage{amsmath,amsthm,amsfonts}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{color}
\usepackage{multicol}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}
\begin{document}

%= http://www.byclb.com/TR/Tutorials/neural_networks/ch4_1.htm
%= https://www.probabilitycourse.sh/chapter9/9_1_8_bayesian_hypothesis_testing.php
\section{Bayesian Hypothesis }

Suppose that we need to decide between two hypotheses $H_0$ and $H_1$. In the Bayesian setting, we assume that we know prior probabilities of $H_0$ and $H_1$. That is, we know $P(H0)=p_0$ and $P(H1)=p_1$, where $p_0+p_1=1$. We observe the random variable (or the random vector) $Y$. We know the distribution of $Y$ under the two hypotheses, i.e, we know
$fY(y|H0)$,and$ fY(y|H1)$.

Using Bayes' rule, we can obtain the posterior probabilities of H0 and H1:
\[P(H0|Y=y)= \frac{fY(y|H0)P(H0)}{fY(y)}\]
\[P(H1|Y=y)= \frac{fY(y|H1)P(H1)}{fY(y)}.\]
One way to decide between H0 and H1 is to compare $P(H0|Y=y)$ and $P(H1|Y=y)$, and accept the hypothesis with the higher posterior probability. This is the idea behind the maximum a posteriori (MAP) test.

Here, since we are choosing the hypothesis with the highest probability, it is relatively easy to show that the error probability is minimized.
To be more specific, according to the MAP test, we choose H0 if and only if

\[P(H_0|Y=y)\geq P(H_1|Y=y).\]
In other words, we choose H0 if and only if
\[fY(y|H0)P(H0)\geq fY(y|H1)P(H1).\]

Note that as always, we use the PMF instead of the PDF if $Y$ is a discrete random variable. We can generalize the MAP test to the case where you have more than two hypotheses. In that case, again we choose the hypothesis with the highest posterior probability.

\begin{framed}
MAP Hypothesis Test
\begin{itemize}
\item Choose the hypothesis with the highest posterior probability, $P(Hi|Y=y)$.
\item Equivalently, choose hypothesis $H_i$ with the highest $fY(y|Hi)P(Hi)$.
\end{itemize}
\end{framed}

\subsection{Example}
\begin{itemize}
    \item Suppose that the random variable XX is transmitted over a communication channel. 
    \item Assume that the received signal is given by
\[Y=X+W,\]
where $W \sim  N(0,\sigma^2)$ is independent of XX. 
\item Suppose that $X=1$ with probability pp, and $X=−1$ with probability $1−p$. 
\item The goal is to decide between $X=1$ and $X=−1$ by observing the random variable $Y$. Find the MAP test for this problem.
\end{itemize}

\subsection{Solution}


Here, we have two hypotheses: 
\begin{description}
\item[H0:] $X=1$,
\item[H1:] $X=−1$.
\end{description}


Under H0, $Y=1+W$, so $Y|H0\sim N(1,\sigma^2)$. Therefore,
\[
\frac{1}{\sigma^2 \sqrt{2\pi} }e^{\frac{−(y−1)^2}{\sigma^2}} 
\]
Under H1, $Y=−1+W$, so $Y|H1\sim N(−1,\sigma^2)$. Therefore,
\[
\frac{1}{\sigma^2 \sqrt{2\pi} }e^{\frac{−(y+1)^2}{\sigma^2}}
\]
Thus, we choose $H_0$ if and only if

\[ \frac{1}{\sigma^2 \sqrt{2\pi} }e^{\frac{−(y−1)^2}{\sigma^2}} P(H0) \geq \frac{1}{\sigma^2 \sqrt{2\pi} }e^{\frac{−(y+1)^2}{\sigma^2}} PP(H1)\]

We have $P(H_0)=p$, and $P(H1)=1−p$. Therefore, we choose $H_0$ if and only if
\[exp(2y\sigma^2)\geq \frac{1−p}{p}.\]
Equivalently, we choose H0 if and only if
\[y\geq \frac{\sigma^2}{2}ln \left(\frac{1−p}{p}\right).\]
%=====================================================================================%

Note that the average error probability for a hypothesis test can be written as
\[Pe=P(choose H1|H0)P(H0)+P(choose H0|H1)P(H1).\]
As we mentioned earlier, the MAP test achieves the minimum possible average error probability.

\subsection*{Example}
Find the average error probability in Example 9.10
Solution


\subsection*{Solution}
in Example 9.10, we arrived at the following decision rule: We choose H0 if and only if $y\geq c$,
where
\[c= \frac{\sigma^2}{2} ln \left(\frac{(1−p)}{p} \right)\]
%================================================%
Since $Y|H0\sim N(1,\sigma^2)$,
\[P(choose H1|H0)=P(Y<c|H0)=\Theta(c−1σ)=\Theta(\sigma^2ln(1−pp)−1σ).\]
Since $Y|H1\sim N(−1,\sigma^2)$,
\[P(choose H0|H1)=P(Y\geq c|H1)=1−\Theta(c+1σ)=1−\Theta(\sigma^2ln(1−pp)+1σ).
\]

Figure 9.4 shows the two error probabilities for this example. Therefore, the average error probability is given by
\[Pe=P(choose H1|H0)P(H0)+P(choose H0|H1)P(H1)=p⋅\Theta(\sigma^2ln(1−pp)−1σ)+(1−p)⋅[1−\Theta(\sigma^2ln(1−pp)+1σ)].\]


error-prob-Bayes-Hyp
Figure 9.4 - Error probabilities for Example 9.10 and Example 9.11

\subsection{Minimum Cost Hypothesis Test:}

Suppose that you are building a sensor network to detect fires in a forest. Based on the information collected by the sensors, the system needs to decide between two opposing hypotheses: 

\begin{description}
\item[H0:] There is no fire,
\itme[H1:] There is a fire.

\end{description}

There are two possible types of errors that we can make: We might accept H0 while H1 is true, or we might accept $H_1$ while $H_0$ is true. Note that the cost associated with these two errors are not the same. In other words, if there is a fire and we miss it, we will be making a costlier error. To address situations like this, we associate a cost to each error type: 

\begin{itemize}
\item[C10:] The cost of choosing H1, given that H0 is true.

\item[C01:] The cost of choosing H0, given that H1 is true.
\end{itemize}
Then, the average cost can be written as
\[C=C10P(choose H1|H0)P(H0)+C01P(choose H0|H1)P(H1).\]
The goal of minimum cost hypothesis testing is to minimise the above expression. Luckily, this can be done easily. Note that we can rewrite the average cost as
\[C=P(choose H1|H0)⋅[P(H0)C10]+P(choose H0|H1)⋅[P(H1)C01].\]
The above expression is very similar to the average error probability of the MAP test (Equation 9.6). The only difference is that we have $p(H0)C10$ instead of P(H0)P(H0), and we have p(H1)C01p(H1)C01 instead of P(H1)P(H1). Therefore, we can use a decision rule similar to the MAP decision rule. 

More specifically, we choose H0 if and only if
\[fY(y|H0)P(H0)C10\geq fY(y|H1)P(H1)C01\]
Here is another way to interpret the above decision rule. If we divide both sides of Equation 9.7 by fY(y)fY(y) and apply Bayes' rule, we conclude the following: We choose H0 if and only if
P(H0|y)C10\geq P(H1|y)C01.
%=========================================================%
\begin{itemize}
    \item Note that P(H0|y)C10 is the expected cost of accepting H1. We call this the posterior risk of accepting H1. 
    \item Similarly, $P(H1|y)C01$ is the posterior risk (expected cost) of accepting H0. 
    \item Therefore, we can summarize the minimum cost test as follows: We accept the hypothesis with the lowest posterior risk.
\end{itemize}

\section{Minimum Cost Hypothesis Test}

Assuming the following costs 

\begin{itemize}
    \item C10: The cost of choosing H1, given that H0 is true.
\item C01: The cost of choosing H0, given that H1 is true.
\end{itemize}

We choose H0 if and only if
\[fY(y|H0)fY(y|H1)\geq P(H1)C01P(H0)C10.\]
Equivalently, we choose H0 if and only if
\[P(H0|y)C10\geq P(H1|y)C01.\]



\subsection{Problem}
Suppose that we need to decide between two opposing hypotheses H0 and H1. Let $Cij$ be the cost of accepting HiHi given that HjHj is true. That is 

\begin{description}
\item[C00:] The cost of choosing H0, given that H0 is true.

\item[C10:] The cost of choosing H1, given that H0 is true.

\item[C01:] The cost of choosing H0, given that H1 is true.

\item[C11:] The cost of choosing H1, given that H1 is true.
\end{itemize}

It is reasonable to assume that the associated cost to a correct decision is less than the cost of an incorrect decision. That is, $c00<c10$ and c11<c01c11<c01. The average cost can be written as\[
C==∑i,jCijP(choose Hi|Hj)P(Hj)C00P(choose H0|H0)P(H0)+C01P(choose H0|H1)P(H1)+C10P(choose H1|H0)P(H0)+C11P(choose H1|H1)P(H1).\]

Our goal is to find the decision rule such that the average cost is minimized. Show that the decision rule can be stated as follows: Choose H0 if and only if
\[fY(y|H0)P(H0)(C10−C00)\geq fY(y|H1)P(H1)(C01−C11)\]
%===================================%
%===================================%
Solution
First, note that
\[P(choose H0|H0)P(choose H1|H1)=1−P(choose H1|H0),=1−P(choose H0|H1).\]

Therefore,
\[
C==  C00 \times [1−P(choose H1|H0)]P(H0)+C01P(choose H0|H1)P(H1)\\
    +C10P(choose H1|H0)P(H0)\\
    +C11[1−P(choose H0|H1)]P(H1)(C10−C00)P(choose H1|H0)P(H0)+(C01−C11)P(choose H0|H1)P(H1)\\
    +C00p(H0)+C11p(H1).
\]
%====================================================%

The term C00p(H0)+C11P(H1)C00p(H0)+C11P(H1) is constant (i.e., it does not depend on the decision rule). Therefore, to minimize the cost, we need to minimize
\[D=P(choose H1|H0)P(H0)(C10−C00)+P(choose H0|H1)P(H1)(C01−C11)\].
%====================================================%
The above expression is very similar to the average error probability of the MAP test (Equation 9.8). The only difference is that we have $p(H0)(C10−C00)p(H0)(C10−C00)$$ instead of $P(H0)$, and we have $p(H1)(C01−C11)p(H1)(C01−C11)$ instead of $P(H1)$$. Therefore, we can use a decision rule similar to the MAP decision rule. More specifically, we choose H0 if and only if
\[fY(y|H0)P(H0)(C10−C00)\geq fY(y|H1)P(H1)(C01−C11)\].
%====================================================%

Example 
A surveillance system is in charge of detecting intruders to a facility. There are two hypotheses to choose from: 
\begin{description}
\item[H0:] No intruder is present.

\item[H1:] There is an intruder.
\end{description}


The system sends an alarm message if it accepts H1. Suppose that after processing the data, we obtain P(H1|y)=0.05P(H1|y)=0.05. Also, assume that the cost of missing an intruder is 1010 times the cost of a false alarm. Should the system send an alarm message (accept H1)?
Solution
First note that
\[P(H0|y)=1−P(H1|y)=0.95\]
The posterior risk of accepting H1 is
\[P(H0|y)C10=0.95C10.\]
We have C01=10C10C01=10C10, so the posterior risk of accepting H0 is
P(H1|y)C01=(0.05)(10C10)=0.5C10.
%====================================================%

Since P(H0|y)C10\geq P(H1|y)C01P(H0|y)C10\geq P(H1|y)C01, we accept H0, so no alarm message needs to be sent.
\end{document}