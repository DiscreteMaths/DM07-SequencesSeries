\documentclass[]{report}

\voffset=-1.5cm
\oddsidemargin=0.0cm
\textwidth = 480pt

\usepackage{framed}
\usepackage{subfiles}
\usepackage{enumerate}
\usepackage{graphics}
\usepackage{newlfont}
\usepackage{eurosym}
\usepackage{amsmath,amsthm,amsfonts}
\usepackage{amsmath}
\usepackage{color}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}
\begin{document}
\section{Cross Validation}
\begin{itemize}
\item
Cross-​​validation is pri­mar­ily a way of mea­sur­ing the pre­dic­tive per­for­mance of a sta­tis­ti­cal model. Every sta­tis­ti­cian knows that the model fit sta­tis­tics are not a good guide to how well a model will pre­dict: high $R^2$ does not nec­es­sar­ily mean a good model. It is easy to over-​​fit the data by includ­ing too many degrees of free­dom and so inflate R^2 and other fit sta­tis­tics. For exam­ple, in a sim­ple poly­no­mial regres­sion I can just keep adding higher order terms and so get bet­ter and bet­ter fits to the data. But the pre­dic­tions from the model on new data will usu­ally get worse as higher order terms are added.
\item
One way to mea­sure the pre­dic­tive abil­ity of a model is to test it on a set of data not used in esti­ma­tion. Data min­ers call this a “test set” and the data used for esti­ma­tion is the “train­ing set”. For exam­ple, the pre­dic­tive accu­racy of a model can be mea­sured by the mean squared error on the test set. This will gen­er­ally be larger than the MSE on the train­ing set because the test data were not used for estimation.
\item
How­ever, there is often not enough data to allow some of it to be kept back for test­ing. A more sophis­ti­cated ver­sion of training/​​test sets is \textit{\textbf{leave-​​one-​​out cross-​​​​validation (LOOCV)}} in which the accu­racy mea­sures are obtained as fol­lows. Sup­pose there are n inde­pen­dent obser­va­tions, y_1,\dots,y_n.
\item
Let obser­va­tion i form the test set, and fit the model using the remain­ing data. Then com­pute the error $(e_{i}^*=y_{i}-\hat{y}_{i})$ for the omit­ted obser­va­tion. This is some­times called a “pre­dicted resid­ual” to dis­tin­guish it from an ordi­nary residual.
Repeat step 1 for $i=1,\dots,n$.
Com­pute the MSE from $e_{1}^*,\dots,e_{n}^*$. We shall call this the CV.
\item
This is a much more effi­cient use of the avail­able data, as you only omit one obser­va­tion at each step. How­ever, it can be very time con­sum­ing to imple­ment (except for lin­ear mod­els — see below).
\item 
Other sta­tis­tics (e.g., the MAE) can be com­puted sim­i­larly. A related mea­sure is the PRESS sta­tis­tic (pre­dicted resid­ual sum of squares) equal to $n \times MSE$.
\end{itemize}
\subsection{variations}
Vari­a­tions on cross-​​validation include leave-​​k-​​out cross-​​validation (in which k obser­va­tions are left out at each step) and k-​​fold cross-​​validation (where the orig­i­nal sam­ple is ran­domly par­ti­tioned into k sub­sam­ples and one is left out in each iter­a­tion). Another pop­u­lar vari­ant is the .632+bootstrap of Efron & Tib­shi­rani (1997) which has bet­ter prop­er­ties but is more com­pli­cated to implement.

Min­i­miz­ing a CV sta­tis­tic is a use­ful way to do model selec­tion such as choos­ing vari­ables in a regres­sion or choos­ing the degrees of free­dom of a non­para­met­ric smoother. It is cer­tainly far bet­ter than pro­ce­dures based on sta­tis­ti­cal tests and pro­vides a nearly unbi­ased mea­sure of the true MSE on new observations.

How­ever, as with any vari­able selec­tion pro­ce­dure, it can be mis­used. Beware of look­ing at sta­tis­ti­cal tests after select­ing vari­ables using cross-​​validation — the tests do not take account of the vari­able selec­tion that has taken place and so the p-​​values can mislead.

It is also impor­tant to realise that it doesn’t always work. For exam­ple, if there are exact dupli­cate obser­va­tions (i.e., two or more obser­va­tions with equal val­ues for all covari­ates and for the y vari­able) then leav­ing one obser­va­tion out will not be effective.

Another prob­lem is that a small change in the data can cause a large change in the model selected. Many authors have found that k-​​fold cross-​​validation works bet­ter in this respect.

In a famous paper, Shao (1993) showed that leave-​​one-​​out cross val­i­da­tion does not lead to a con­sis­tent esti­mate of the model. That is, if there is a true model, then LOOCV will not always find it, even with very large sam­ple sizes. In con­trast, cer­tain kinds of leave-​​k-​​out cross-​​validation, where k increases with n, will be con­sis­tent. Frankly, I don’t con­sider this is a very impor­tant result as there is never a true model. In real­ity, every model is wrong, so con­sis­tency is not really an inter­est­ing property.

\newpage
\subsection{Cross Validation}

Cross-validation, sometimes called rotation estimation,is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set.
\begin{itemize}
\item It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. It is worth highlighting that in a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (testing dataset).
\item The goal of cross validation is to define a dataset to "test" the model in the training phase (i.e., the validation dataset), in order to limit problems like overfitting, give an insight on how the model will generalize to an independent data set (i.e., an unknown dataset, for instance from a real problem), etc.
\end{itemize}

\subsection{Cross Validation}
%-------------------------------------------------------------------------------------%

The confusion
table is a table in which the rows are the observed categories of the dependent and
the columns are the predicted categories. When prediction is perfect all cases will lie on the
diagonal. The percentage of cases on the diagonal is the percentage of correct classifications. 
The cross validated set of data is a more honest presentation of the power of the
discriminant function than that provided by the original classifications and often produces
a poorer outcome. The cross validation is often termed a jack-knife classification, in that
it successively classifies \textbf{all cases but one} to develop a discriminant function and then
categorizes the case that was left out. This process is repeated with each case left out in
turn.This is known as leave-1-out cross validation. 

This cross validation produces a more reliable function. The argument behind it is that
one should not use the case you are trying to predict as part of the categorization process.


%-----------------------------------------------------------------------------------%
\subsection{Error Rates}

We can evaluate error rates by means of a training sample (to construct the discrimination rule) and a test sample.


An optimistic error rate is obtained by reclassifying the training data. (In the \textbf{\textit{training data}} sets, how many cases were misclassified). This is known as the \textbf{apparent error rate}.


The apparent error rate is obtained by using in the training set to estimate
the error rates. It can be severely optimistically biased, particularly for complex classifiers, and in the presence of over-fitted models.


If an independent test sample is used for classifying, we arrive at the  \textbf{true error rate}.The true error rate (or conditional error rate) of a classifier is the expected
probability of misclassifying a randomly selected pattern.
It is the error rate of an infinitely large test set drawn from the same distribution as the training data.



%---------------------------------------------------------------------------------------%

\subsection{Misclassification Cost}

As in all statistical procedures it is helpful to use diagnostic procedures to asses the efficacy of the discriminant analysis. We use \textbf{cross-validation} to assess the classification probability.
Typically you are going to have some prior rule as to what is an \textbf{acceptable misclassification rate}.

Those rules might involve things like, ``what is the cost of misclassification?" Consider a medical study where you might be able to diagnose cancer.

There are really two alternative costs. The cost of misclassifying someone as having cancer when they don't.
This could cause a certain amount of emotional grief. Additionally there would be the substantial cost of unnecessary treatment.

There is also the alternative cost of misclassifying someone as not having cancer when in fact they do have it.

A good classification procedure should
 \begin{itemize}
 \item result in few misclassifications
 \item take \textbf{\textit{prior probabilities of occurrence}} into account
 \item consider the cost of misclassification
 \end{itemize}
 
For example, suppose there tend to be more financially sound firms than bankrupt
firm. If we really believe that the prior probability of a financially
distressed and ultimately bankrupted firm is very small, then one should
classify a randomly selected firm as non-bankrupt unless the data
overwhelmingly favor bankruptcy.



There are two costs associated with discriminant analysis classification: The true misclassification cost per class, and the expected misclassification cost (ECM) per observation.

Suppose there we have a binary classification system, with two classes: class 1 and class 2.
Suppose that classifying a class 1 object as belonging to class 2 represents a more serious error than classifying a class 2 object as belonging to class 1. There would an assignable cost to each error.
$c(i|j)$ is the cost of classifying an observation into class $j$ if its true class is $i$.
The costs of misclassification can be defined by a cost matrix.

\begin{tabular}{|c|c|c|}
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  & Predicted & Predicted \\
   & Class 1 & Class 2 \\  \hline
  Class 1 & 0 & $c(2|1)$  \\
  Class 2 & $c(1|2)$ & 0 \\
  \hline
\end{tabular}




\subsection{Expected cost of misclassification (ECM)}
Let $p_1$ and $p_2$ be the prior probability of class 1 and class 2 respectively.
Necessarily $p_1$ + $p_2$ = 1.

The conditional probability of classifying an object as class 1 when it is in fact from
class 2 is denoted $p(1|2)$.
Similarly the conditional probability of classifying an object as class 2 when it is in
fact from class 1 is denoted $p(2|1)$.

\[ECM = c(2|1)p(2|1)p_1 + c(1|2)p(1|2)p_2\]
(In other words: the sum of the cost of misclassification times the (joint) probability of that misclassification.

A reasonable classification rule should have ECM as small as possible.







\end{document}


%---------------------------%
\section{Training data sets}

A training set is a set of data used in various areas of information science to discover potentially predictive relationships. Training sets are used in artificial intelligence, machine learning, genetic programming, intelligent systems, and statistics. In all these fields, a training set has much the same role and is often used in conjunction with a test set.

\end{document}
%=====================%
